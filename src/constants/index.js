export const myProjects = [
  {
    id: 1,
    title: "Demcus",
    description:
      "Waveform to waveform model.",
    subDescription: [
      "Developed a waveform-to-waveform model for audio source separation using Wave-U-Net architecture.",
      "Utilized MUSDB HQ dataset for training and evaluation, achieving high-quality audio separation.",
      "Implemented the model in Python with PyTorch, ensuring efficient training and inference.",
      "Contributed to open-source repositories, enhancing accessibility for researchers and developers in the field.",
    ],
    href: "",
    logo: "",
    image: "/assets/projects/demcuss.png",
    tags: [
      {
        id: 1,
        name: "⚠️ 50 seconds",
        path: "/assets/logos/time60.svg",
      },
      {
        id: 2,
        name: "Wave-U-Net",
        path: "/assets/logos/histogram.svg",
      },
      {
        id: 3,
        name: "MUSDB HQ",
        path: "/assets/logos/signal.svg",
      },
      {
        id: 4,
        name: "PyPI",
        path: "/assets/logos/python.svg",
      },
    ],
  },
  {
    id: 2,
    title: "ConvTasNet",
    description:
      "Convolution time-domain audio separation model.",
    subDescription: [
      "Developed a convolutional time-domain audio separation model using ConvTasNet architecture.",
      "Utilized MUSDB HQ dataset for training and evaluation, achieving state-of-the-art performance in speech separation tasks.",
      "Implemented the model in Python with PyTorch, ensuring efficient training and inference.",
      "Contributed to open-source repositories, enhancing accessibility for researchers and developers in the field.",
    ],
    href: "",
    logo: "",
    image: "/assets/projects/convtasnet.png",
    tags: [
      {
        id: 1,
        name: "⚠️ 6 seconds",
        path: "/assets/logos/time10.svg",
      },
      {
        id: 2,
        name: "Speech",
        path: "/assets/logos/speech.svg",
      },
      {
        id: 3,
        name: "Time-Freq",
        path: "/assets/logos/frequency.svg",
      },
      {
        id: 4,
        name: "TorchAudio",
        path: "/assets/logos/python.svg",
      },
    ],
  },
];

export const mySocials = [
  {
    name: "WhatsApp",
    href: "https://wa.me/919478972509?text=Hello!%20I'm%20interested%20in%20knowing%20more%20about%20neuroxones!",
    icon: "/assets/socials/whatsApp.svg",
  },
  {
    name: "Linkedin",
    href: "https://www.linkedin.com/in/lagooneration/",
    icon: "/assets/socials/linkedIn.svg",
  },
  {
    name: "Instagram",
    href: "https://www.instagram.com/lagooneration",
    icon: "/assets/socials/instagram.svg",
  },
];

export const experiences = [
  {
    title: "Primary Research",
    job: "Understanding Auditory Attention",
    date: "2021-2022",
    contents: [
      "Inspired by research done by Bertrand A, KU Leuven on auditory attention and mechanisms responsible for neuro-steered attention detection.",
  ],
  },
  {
    title: "Conceptualization",
    job: "Neuro-Steered Device",
    date: "2022-2024",
    contents: [
      "Conceptualized a prototype for neuro-steered EEG extension, starting with auditory attention detection.",
      "Prototyping plan and research for suitable equipment and hardware for EEG extension compatible with AR/VR devices.",
    ],
  },
  {
    title: "Algorithm Development",
    job: "Auditory Attention Detection",
    date: "2024-Present",
    contents: [
      "Developing algorithms to detect auditory attention using EEG data.",
      "Implementing machine learning techniques to improve accuracy and responsiveness.",
      "Collaborating with researchers to validate and refine the detection algorithms.",
    ],
  },
];
export const reviews = [
  {
    name: "Working Prototype",
    username: "@Studio",
    body: "Prepration to use mBrainTrain Mobile EEG by BRAINBOX.",
    img: "/assets/logos/prototype.svg",
  },
  {
    name: "Machine Learning",
    username: "@AI_&_ML",
    body: "Using machine learning on EEG data for auditory scene analysis.",
    img: "/assets/logos/ml.svg",
  },
  {
    name: "Materials & Equipment",
    username: "@Procurement",
    body: "Procuring best suitable tech for compact user-centric design.",
    img: "/assets/logos/material.svg",
  },
  {
    name: "Patents & IP",
    username: "@Legal",
    body: "Patenting the neuro-steered plugon design and technology with AR/VR consoles.",
    img: "/assets/logos/patent.svg",
  },
  {
    name: "Business Development",
    username: "@BizDev",
    body: "Exploring partnerships with tech companies for product integration.",
    img: "/assets/logos/business.svg",
  },
  {
    name: "Clinical Trials",
    username: "@Clinical",
    body: "Planning to test the prototype in clinical settings on auditory impared.",
    img: "/assets/logos/medical.svg",
  },
  {
    name: "Sensory Enhancement",
    username: "@Feature",
    body: "Checking for enhanced listening sound waves beyond human capabilities.",
    img: "https://robohash.org/dave",
  },
  {
    name: "User Experience",
    username: "@UX_Design",
    body: "Designing user-friendly interfaces for the neuro-steered headphones.",
    img: "https://robohash.org/eve",
  },
];




export const words = [
  { text: "Neuro-Steered", imgPath: "/images/ideas.svg" },
  { text: "Integrated EEG", imgPath: "/images/concepts.svg" },
  { text: "Attention-Based", imgPath: "/images/designs.svg" },
  { text: "AI/ML", imgPath: "/images/code.svg" },
  { text: "Neuro-Steered", imgPath: "/images/ideas.svg" },
  { text: "Integrated EEG", imgPath: "/images/concepts.svg" },
  { text: "Attention-Based", imgPath: "/images/designs.svg" },
  { text: "AI/ML", imgPath: "/images/code.svg" },
];









