export const myProjects = [
  {
    id: 1,
    title: "Demcus",
    description:
      "Waveform to waveform model.",
    subDescription: [
      "Developed a waveform-to-waveform model for audio source separation using Wave-U-Net architecture.",
      "Utilized MUSDB HQ dataset for training and evaluation, achieving high-quality audio separation.",
      "Implemented the model in Python with PyTorch, ensuring efficient training and inference.",
      "Contributed to open-source repositories, enhancing accessibility for researchers and developers in the field.",
    ],
    href: "",
    logo: "",
    image: "/assets/projects/demcuss.png",
    tags: [
      {
        id: 1,
        name: "⚠️ 50 seconds",
        path: "/assets/logos/time60.svg",
      },
      {
        id: 2,
        name: "Wave-U-Net",
        path: "/assets/logos/histogram.svg",
      },
      {
        id: 3,
        name: "MUSDB HQ",
        path: "/assets/logos/signal.svg",
      },
      {
        id: 4,
        name: "PyPI",
        path: "/assets/logos/python.svg",
      },
    ],
  },
  {
    id: 2,
    title: "ConvTasNet",
    description:
      "Convolution time-domain audio separation model.",
    subDescription: [
      "Developed a convolutional time-domain audio separation model using ConvTasNet architecture.",
      "Utilized MUSDB HQ dataset for training and evaluation, achieving state-of-the-art performance in speech separation tasks.",
      "Implemented the model in Python with PyTorch, ensuring efficient training and inference.",
      "Contributed to open-source repositories, enhancing accessibility for researchers and developers in the field.",
    ],
    href: "",
    logo: "",
    image: "/assets/projects/convtasnet.png",
    tags: [
      {
        id: 1,
        name: "⚠️ 6 seconds",
        path: "/assets/logos/time10.svg",
      },
      {
        id: 2,
        name: "Speech",
        path: "/assets/logos/speech.svg",
      },
      {
        id: 3,
        name: "Time-Freq",
        path: "/assets/logos/frequency.svg",
      },
      {
        id: 4,
        name: "TorchAudio",
        path: "/assets/logos/python.svg",
      },
    ],
  },
];

export const mySocials = [
  {
    name: "WhatsApp",
    href: "https://wa.me/919478972509?text=Hello!%20I'm%20interested%20in%20knowing%20more%20about%20neuroxones!",
    icon: "/assets/socials/whatsApp.svg",
  },
  {
    name: "Linkedin",
    href: "https://www.linkedin.com/in/lagooneration/",
    icon: "/assets/socials/linkedIn.svg",
  },
  {
    name: "Instagram",
    href: "https://www.instagram.com/lagooneration",
    icon: "/assets/socials/instagram.svg",
  },
];

export const experiences = [
  {
    title: "",
    job: "Attention Decoding",
    date: "Limitations",
    contents: [
      "A 'decision window' of 1-2 seconds to detect attention shifts, creating a noticeable lag in real-world applications.",
      "visualizer: true",
    ],
  },
  {
    title: "",
    job: "Audio Source Separation",
    date: "Real-Time",
    contents: [
      "Recent studies suggest that integrating neural signals with speech enhancement algorithms represents a promising research direction.",
      "youtubeUrl: https://www.youtube.com/embed/_wPZ2l12C-o",
    ],
  },  {
    title: "",
    job: "Capture it!",
    date: "Don't just remove noise",
    contents: [
      "Investigating the feasibility & performance to capture clean EEG signals using physiological artifacts prediction learning framework for real-time EEG noise cancellation.",
      "imagePath: /assets/images/hp.png",
    ],
  },
];
export const reviews = [
  {
    name: "Working Prototype",
    username: "@Studio",
    body: "Prepration to use mBrainTrain Mobile EEG by BRAINBOX.",
    img: "/assets/logos/prototype.svg",
  },
  {
    name: "Machine Learning",
    username: "@AI_&_ML",
    body: "Using machine learning on EEG data for auditory scene analysis.",
    img: "/assets/logos/ml.svg",
  },
  {
    name: "Materials & Equipment",
    username: "@Procurement",
    body: "Procuring best suitable tech for compact user-centric design.",
    img: "/assets/logos/material.svg",
  },
  {
    name: "Patents & IP",
    username: "@Legal",
    body: "Patenting the neuro-steered design and technology compatible with headphones and AR/VR headsets.",
    img: "/assets/logos/patent.svg",
  },
  {
    name: "Business Development",
    username: "@BizDev",
    body: "Exploring partnerships with tech companies for product integration.",
    img: "/assets/logos/business.svg",
  },
  {
    name: "Clinical Trials",
    username: "@Clinical",
    body: "Planning to test the prototype in clinical settings on auditory impared.",
    img: "/assets/logos/medical.svg",
  },
  {
    name: "Sensory Enhancement",
    username: "@Feature",
    body: "Checking for enhanced listening sound waves beyond human capabilities.",
    img: "https://robohash.org/dave",
  },
  {
    name: "User Experience",
    username: "@UX_Design",
    body: "Designing user-friendly interfaces for the neuro-steered headphones.",
    img: "https://robohash.org/eve",
  },
];




export const words = [
  { text: "Neuro-Steered", imgPath: "/images/ideas.svg" },
  { text: "Integrated EEG", imgPath: "/images/concepts.svg" },
  { text: "Attention-Based", imgPath: "/images/designs.svg" },
  { text: "AI/ML", imgPath: "/images/code.svg" },
  { text: "Neuro-Steered", imgPath: "/images/ideas.svg" },
  { text: "Integrated EEG", imgPath: "/images/concepts.svg" },
  { text: "Attention-Based", imgPath: "/images/designs.svg" },
  { text: "AI/ML", imgPath: "/images/code.svg" },
];









